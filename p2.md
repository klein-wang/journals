## ML | PlanGAN: Model-based Planning With Sparse Rewards and Multiple Goals

***PlanGAN: Model-based Planning With Sparse Rewards and Multiple Goals***

Authors: Henry Charlesworth, Giovanni Montana

Link: [https://arxiv.org/abs/2006.00900](https://link.zhihu.com/?target=https%3A//arxiv.org/abs/2006.00900)

#### Abstract

> Learning with sparse rewards remains a significant challenge in reinforcement learning (RL), especially when the aim is to train a policy capable of achieving multiple different goals. To date, the most successful approaches for dealing with multi-goal, sparse reward environments have been model-free RL algorithms. In this work we propose PlanGAN, a model-based algorithm specifically designed for solving multi-goal tasks in environments with sparse rewards. Our method builds on the fact that any trajectory of experience collected by an agent contains useful information about how to achieve the goals observed during that trajectory. We use this to train an ensemble of conditional generative models (GANs) to generate plausible trajectories that lead the agent from its current state towards a specified goal. We then combine these imagined trajectories into a novel planning algorithm in order to achieve the desired goal as efficiently as possible. The performance of PlanGAN has been tested on a number of robotic navigation/manipulation tasks in comparison with a range of model-free reinforcement learning baselines, including Hindsight Experience Replay. Our studies indicate that PlanGAN can achieve comparable performance whilst being around 4-8 times more sample efficient.

#### Introduction

In this paper the authors present PlanGAN, a model-based algorithm that can naturally be applied to sparse reward environments with multiple goals. The core of this method builds upon the same principle that underlies HER — namely that any goal observed during a given trajectory can be used as an example of how to achieve that goal from states that occurred earlier on in that same trajectory. 

However, unlike HER, the algorithm does not directly learn a goal-conditioned policy/value function but rather train an ensemble of Generative Adversarial Networks (GANs) which learn to generate plausible future trajectories conditioned on achieving a particular goal. Then these imagined trajectories are combined into a novel planning algorithm that can reach those goals in an efficient manner.

#### Methodology

The aim of the first major component of the methodology is to train a generative model that can take in the current state ![[公式]](https://www.zhihu.com/equation?tex=s_t) along with a desired goal ![[公式]](https://www.zhihu.com/equation?tex=g) and produce an imagined action at and next state ![[公式]](https://www.zhihu.com/equation?tex=s_%7Bt%2B1%7D) that moves the agent towards achieving ![[公式]](https://www.zhihu.com/equation?tex=g) .

Intuitionally, the authors want to use the model to take a state-action pair ![[公式]](https://www.zhihu.com/equation?tex=%28s_t%2C+a_t%29) and predict the difference between the next state and current state, ![[公式]](https://www.zhihu.com/equation?tex=s_%7Bt%2B1%7D-s_t) . The predictive models in the paper are used to provide an L2 regularisation (Ridge) term in the generator loss that encourages the generated actions and next states to be consistent with the predictions of the one-step models. 

![img](https://pic3.zhimg.com/80/v2-e428832189cfaff49e0b58d32a048efe_1440w.jpg)

The basic structure of the planner is to make use of a model to generate a number of imaginary future trajectories, score them, use these scores to choose the next action, and repeat this whole procedure at the next step. The score captures how effective those trajectories are in terms of moving towards the final goal ![[公式]](https://www.zhihu.com/equation?tex=g) . A good score here should reflect the fact that we want the next action to be moving us towards ![[公式]](https://www.zhihu.com/equation?tex=g) as quickly as possible whilst also ensuring that the goal can be retained at later time steps.

Once these trajectories have been generated, the researchers give each of them a score based on the *fraction of time they spend achieving the goal*. This means that trajectories that reach the goal quickly are scored highly, but only if they are able to remain there. Accordingly, trajectories that do not reach the goal within T steps are given a score of zero. They can then score each of the initial actions ![[公式]](https://www.zhihu.com/equation?tex=%28a%5Eq_t%29%5EQ_%7Bq%3D1%7D) based on the average score of all the imagined trajectories that started with that action. These scores are normalised and denoted as ![[公式]](https://www.zhihu.com/equation?tex=n_i) . The final action returned by the planner is either the action with the maximum score or an exponentially weighted average of the initially proposed actions, ![[公式]](https://www.zhihu.com/equation?tex=a_t%3D%5Cfrac%7B%5Csum_%7Bi%3D1%7D%5E%7BR%7D%7Be%5E%7B%5Calpha+n_i%7D%5Calpha_i%7D%7D%7B%5Csum_%7Bj%3D1%7D%5E%7BQ%7D%7Be%5E%7B%5Calpha+n_i%7D%7D%7D) , where ![[公式]](https://www.zhihu.com/equation?tex=%5Calpha%3E0) is a hyperparameter. 

![img](https://pic1.zhimg.com/80/v2-f6288903a114299b9de194b129be7598_1440w.jpg)