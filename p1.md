## ML | Hindsight Experience Replay

***Hindsight Experience Replay***

Authors: Marcin Andrychowicz∗ , Filip Wolski, Alex Ray, Jonas Schneider, Rachel Fong, Peter Welinder, Bob McGrew, Josh Tobin, Pieter Abbeel† , Wojciech Zaremba†

Link: [https://arxiv.org/abs/1707.01495](https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1707.01495)

#### Abstract

> Dealing with sparse rewards is one of the biggest challenges in Reinforcement Learning (RL). We present a novel technique called Hindsight Experience Replay which allows sample-efficient learning from rewards which are sparse and binary and therefore avoid the need for complicated reward engineering. It can be combined with an arbitrary off-policy RL algorithm and may be seen as a form of implicit curriculum. We demonstrate our approach on the task of manipulating objects with a robotic arm. In particular, we run experiments on three different tasks: pushing, sliding, and pick-and-place, in each case using only binary rewards indicating whether or not the task is completed. Our ablation studies show that Hindsight Experience Replay is a crucial ingredient which makes training possible in these challenging environments. We show that our policies trained on a physics simulation can be deployed on a physical robot and successfully complete the task. The video presenting our experiments is available at [https://goo.gl/SMrQnI](https://link.zhihu.com/?target=https%3A//goo.gl/SMrQnI).

#### Introduction

HER (Hindsight Experience Replay) is introduced to allow the algorithm to learn almost as much from achieving an undesired outcome as from the desired one, as humans do. Unlike the current generation of model-free RL algorithms, HER makes the learning possible even if the reward signal is unshaped (i.e. sparse and binary). 

In many cased of reinforcement learning, we often need to augment the reward using domain knowledge, in what is known as Reward Engineering or Reward Shaping. The problem here is that it is not always practically workable to discover the proper shaping for the reward functions. In other words, the domain knowledge required for strengthening learning is not always available. 

#### Example & Methodology

In this paper, the authors have given out a motivating example, which asks us to consider a bit-flipping environment (page3) with state space ![[公式]](https://www.zhihu.com/equation?tex=S%3D%280%2C1%29%5En) , action space ![[公式]](https://www.zhihu.com/equation?tex=A%3D%7B0%2C1%2C...%2Cn-1%7D)for some integer n in which executing the i-th action flips the i-th bit of the state. The policy gets a reward of -1 as long as it is not in the target state, i.e. ![[公式]](https://www.zhihu.com/equation?tex=r_g%28s%2Ca%29%3D-%5Bs%5Cne+g%5D) . 

As mentioned in the example, a standard solution to this problem would be to use a shaped reward function which is more informative and guides the agent towards the goal, e.g. ![[公式]](https://www.zhihu.com/equation?tex=r_g%28s%2Ca%29%3D-%7C%7Cs-g%7C%7C%5E2) . But this approach may be difficult to apply to more complicated problems (i.e. hard to capture the full information using shaped reward function like above). 

The second approach that has been raised is HER, which the reasoning is explained below:

> The pivotal idea behind our approach is to re-examine this trajectory with a different goal — while this trajectory may not help us learn how to achieve the state ![[公式]](https://www.zhihu.com/equation?tex=g) , it definitely tells us something about how to achieve the state ![[公式]](https://www.zhihu.com/equation?tex=s_T).

**Off-Policy Learning**

But how do human deal with such problem using HER? Sometimes when we fail to perform some tasks, we recognize that what we have done could be useful in another context, or for another task. It is the intuition that the authors of the paper used to develop their method. 

In HER, the authors suggest the following strategy: suppose our agent performs an episode of trying to reach goal state G from initial state S, but fails to do so and ends up in some state S' at the end of the episode. We cache the trajectory into our replay buffer where ![[公式]](https://www.zhihu.com/equation?tex=r_k)is the reward received at step k of the episode, and![[公式]](https://www.zhihu.com/equation?tex=a_k)is the action taken at step k of the episode.

The idea in HER is to **imagine that our goal has actually been S' all along**, and that in this alternative reality our agent has reached the goal successfully and got the positive reward for doing so.

In addition to caching the real trajectory, we also cache the trajectory with imagined goal S'. This trajectory is motivated by the human ability to learn useful things from failed attempts. By introducing the imagined trajectories to our replay buffer, we ensure that**no matter how bad our policy is, it will always have some positive rewards to learn from**.

The magic of function approximation by neural networks will ensure that our policy could also reach states similar to those it has seen before; this is the generalization property that is the hallmark of successful deep learning. At first, the agent will be able to reach states in a relatively small area around the initial state, but gradually it expands this reachable area of the state space until finally it learns to reach those goal states we are actually interested in.

![Pseudo Code of HER](https://pic1.zhimg.com/80/v2-ff722eecbf0f8f8ef16586d10451d288_1440w.jpg)

> HER may be seen as a form of implicit curriculum as the goals used for replay naturally shift from ones which are simple to achieve even by a random agent to more difficult ones. However, in contrast to explicit curriculum, HER does not require having any control over the distribution of initial environment states.

