---
layout: journal
title: Attention Is All You Need
date: 2021-08-17 16:40:20
tags: [NLP, Machine Learning, Self-Attention, Transformer]
categories:
- [academics, journal review]

description: We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. The model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.
---

**Attention Is All You Need**

Authors: Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin

Link: [https://arxiv.org/abs/1706.03762](https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1706.03762)

## Abstract

> The dominant sequence transduction models are based on complex recurrent or convolutional neural networks (RNN, CNN) in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. 
>
> We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.

## Limitation of RNN model

Recurrent models typically factor computation along the symbol positions of the input and output sequences. Aligning the positions to steps in computation time, they generate a sequence of hidden states ![](https://www.zhihu.com/equation?tex=h_t+) , as a function of the previous hidden state ![](https://www.zhihu.com/equation?tex=h_%7Bt-1%7D) and the input for position t. This inherently sequential nature precludes parallelization within training examples, which becomes critical at longer sequence lengths, as memory constraints limit batching across examples. 

## Why Self-Attention

First advantage is the total computational complexity per layer. When it comes to self-attention, the amount of computation can be parallelized, as measured by the minimum number of sequential operations required. 

![](https://pic2.zhimg.com/80/v2-3ebc69bc8606497dc9ee9e2f89b99c3d_1440w.jpg)

Also, as a self-attention layer connects all positions with a constant number of sequentially executed operations, the overall model improves the difficulties in reducing path length in long-range dependencies in the network.

## Model Architecture (Transformer)

Instead of paying attention to the last state of the encoder as is usually done with RNNs, in each step of the decoder we look at all the states of the encoder, being able to access information about all the elements of the input sequence. This is what attention does, it extracts information from the whole sequence, a weighted sum of all the past encoder states. This allows the decoder to assign greater weight or importance to a certain element of the input for each element of the output.

Within the self-attention mechanism of the model, there are 3 elements being introduced in the paper: **The Query, The Value and The Key**. The model is computing the dot product of the query with all keys, divide each by the square root of dk, and apply a softmax function to obtain the weights on the values.

![](https://www.zhihu.com/equation?tex=Attention%28Q%2CK%2CV%29%3Dsoftmax%28%5Cfrac%7BQK%5ET%7D%7B%5Csqrt%7Bd_k%7D%7D%29V)

The attention scores measure how much focus to place on other places or words of the input sequence w.r.t a word at a certain position. That is, the dot product of the query vector with the key vector of the respective word we're scoring. 

**Multi-head Attention**

In the previous description the attention scores are focused on the whole sentence at a time, this would produce the same results even if two sentences contain the same words in a different order. Instead, authors in the paper proposed Multi-head Attention, which they added attention to different segments of the words. 

> "We can give the self-attention greater power of discrimination, by combining several self-attention heads, dividing the words vectors into a fixed number (h, number of heads) of chunks, and then self-attention is applied on the corresponding chunks, using Q, K and V sub-matrices.", Peter Bloem. 

![](https://pic4.zhimg.com/80/v2-fb47315cfee387e13803197ae64ab19b_1440w.jpg)

After calculating the dot product of every head, we concatenate the output matrices and multiply them by an additional weights matrix. This final matrix captures information from all the attention heads.

![](https://www.zhihu.com/equation?tex=MultiHead%28Q%2CK%2CV%29%3DConcat%28head_1%2C+...%2C+head_h%29W%5EO%2C+)

![](https://www.zhihu.com/equation?tex=%5Cmbox%7Bwhere+%7D+head_i%3DAttention%28QW%5EQ_i%2CKW%5EK_i%2CVW%5EV_i%29)

The paper also addressed positional encoding approach to solve the issue with the order of the words in the sentence. Briefly speaking, the same scores would be computed even if we shuffle up the words in an input sentence, due to permutation invariant characteristic of the self-attention mechanism. 

The authors has applied a function to map the position in the sentence to a real valued vector. The network will then learn how to use this information. Another approach would be to use a position embedding, similar to word embedding, coding every known position with a vector.

Next, a residual connection is introduced around each sub-layer (attention and Fully connected fee-forward network, summing up the output of the layer with its input, followed by a layer normalization. Before every residual connection, a regularization is applied: 

"We apply dropout to the output of each sub-layer, before it is added to the sub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the positional encodings in both the encoder and decoder stacks" with a dropout rate of 0.1.

The figure below shows the components detailed:

![](https://pic2.zhimg.com/80/v2-9d0bc477be197df621aca2ed01a69e5d_1440w.jpg)

There are many other details which I haven't touched on here. Feel free to discuss more thoughts in this open space . The paper offers an excellent opportunity to people who are interested in technical innovation on sequence-sequence translation tasks. The article in **towardsdatascience** also did a great job in explaining the transformer model regarding this paper. You can read it from [here](https://towardsdatascience.com/attention-is-all-you-need-discovering-the-transformer-paper-73e5ff5e0634). 

## Conclusion

The model performance was evaluated by looking at the BLEU score on the English-to-German and English-to-French newstest2014 tests as a fraction of the training cost. 

![](https://pic3.zhimg.com/80/v2-5291c76e7a50ba517ecba226297e4ebe_1440w.jpg)

The table here summarizes the results and compares models' translation quality and training costs to other model architectures from the literature. 

For translation tasks, the Transformer can be trained significantly faster than architectures based on recurrent or convolutional layers, specifically on both WMT 2014 English-to-German and WMT 2014 English-to-French translation tasks.